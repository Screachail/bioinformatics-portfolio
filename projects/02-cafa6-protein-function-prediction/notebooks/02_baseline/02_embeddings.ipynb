{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11657f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/screachail/projects/bioinformatics-portfolio/projects/02-cafa6-protein-function-prediction/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Generowanie embedding√≥w TRAIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2576/2576 [19:52<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Generowanie embedding√≥w TEST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7010/7010 [50:38<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gotowe! Masz pliki .npy na dysku.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ‚ö†Ô∏è UWAGA: Ten notebook nie musi byƒá uruchamiany ponownie!\n",
    "# Embeddingi sƒÖ ju≈º wygenerowane w data/gold/\n",
    "# Uruchom tylko je≈õli chcesz regenerowaƒá embeddingi (>60 minut)\n",
    "\n",
    "if os.path.exists(\"../data/gold/X_train_esm2.npy\") and os.path.exists(\"../data/gold/X_test_esm2.npy\"):\n",
    "    print(\"‚úÖ Embeddingi ju≈º istniejƒÖ! Pomijam...\")\n",
    "    print(\"   X_train_esm2.npy\")\n",
    "    print(\"   X_test_esm2.npy\")\n",
    "    print(\"\\nüí° Mo≈ºesz pominƒÖƒá ten notebook i przej≈õƒá do 03_training_nn.ipynb\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Brak embedding√≥w, bƒôdƒô je generowaƒá...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "def generate_embeddings(fasta_path, output_npy, ids_output_npy):\n",
    "    \"\"\"Generuje embeddingi z error handlingiem\"\"\"\n",
    "    \n",
    "    # Wczytaj sekwencje\n",
    "    records = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for r in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        prot_id = r.id.split('|')[1] if '|' in r.id else r.id\n",
    "        seq = str(r.seq)\n",
    "        \n",
    "        # üÜï DODAJ: Filtrowanie zbyt d≈Çugich sekwencji\n",
    "        if len(seq) > 1024:\n",
    "            seq = seq[:1024]\n",
    "            skipped += 1\n",
    "        \n",
    "        # üÜï DODAJ: Usuwanie nietypowych aminokwas√≥w\n",
    "        seq = ''.join([aa if aa in 'ACDEFGHIKLMNPQRSTVWY' else 'X' for aa in seq])\n",
    "        \n",
    "        records.append({\"ID\": prot_id, \"Seq\": seq})\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    print(f\"üìä Statystyki:\")\n",
    "    print(f\"  Bia≈Çka: {len(df):,}\")\n",
    "    print(f\"  Obciƒôte (>1024aa): {skipped}\")\n",
    "    print(f\"  ≈örednia d≈Çugo≈õƒá: {df['Seq'].str.len().mean():.1f} aa\")\n",
    "    \n",
    "    # Generowanie embedding√≥w\n",
    "    all_embs = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    # üÜï DODAJ: Progress bar z dodatkowƒÖ informacjƒÖ\n",
    "    pbar = tqdm(range(0, len(df), batch_size), desc=\"Embedding batches\")\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch_seqs = df['Seq'].iloc[i : i + batch_size].tolist()\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch_seqs, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=1024\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                all_embs.append(emb)\n",
    "            \n",
    "            # üÜï DODAJ: Update progress bar z info\n",
    "            pbar.set_postfix({\n",
    "                'batch': f'{i//batch_size + 1}/{len(df)//batch_size + 1}',\n",
    "                'proteins': f'{min(i + batch_size, len(df))}/{len(df)}'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error w batchu {i}: {e}\")\n",
    "            # Spr√≥buj pojedynczo\n",
    "            for seq in batch_seqs:\n",
    "                try:\n",
    "                    inputs = tokenizer([seq], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                        all_embs.append(emb)\n",
    "                except:\n",
    "                    # Je≈õli nadal b≈ÇƒÖd, u≈ºyj zerowych embedding√≥w\n",
    "                    all_embs.append(np.zeros((1, 320)))\n",
    "    \n",
    "    # Zapis\n",
    "    final_embs = np.vstack(all_embs)\n",
    "    np.save(output_npy, final_embs)\n",
    "    np.save(ids_output_npy, df['ID'].values)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Zapisano embeddingi: {final_embs.shape}\")\n",
    "\n",
    "# Reszta bez zmian\n",
    "print(\"üß¨ Generowanie embedding√≥w TRAIN...\")\n",
    "generate_embeddings(\n",
    "    \"../data/bronze/Train/train_sequences.fasta\", \n",
    "    \"../data/gold/X_train_esm2.npy\", \n",
    "    \"../data/gold/train_ids.npy\"\n",
    ")\n",
    "\n",
    "print(\"\\nüß¨ Generowanie embedding√≥w TEST...\")\n",
    "generate_embeddings(\n",
    "    \"../data/bronze/Test/testsuperset.fasta\", \n",
    "    \"../data/gold/X_test_esm2.npy\", \n",
    "    \"../data/gold/test_protein_ids.npy\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Gotowe! Wszystkie embeddingi zapisane.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
